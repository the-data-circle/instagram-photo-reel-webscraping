{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729a2f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import dependencies\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.wait import WebDriverWait\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.common.exceptions import NoSuchElementException, TimeoutException\n",
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import config\n",
    "import json\n",
    "import os\n",
    "from urllib.parse import urlparse\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833b157f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#specify the path to chromedriver.exe (download and save on your computer)\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "#open the webpage\n",
    "driver.get(\"https://www.instagram.com/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe88437",
   "metadata": {},
   "outputs": [],
   "source": [
    "#target username\n",
    "username = WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.CSS_SELECTOR, \"input[name='username']\")))\n",
    "password = WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.CSS_SELECTOR, \"input[name='password']\")))\n",
    "\n",
    "#enter username and password\n",
    "username.clear()\n",
    "username.send_keys(config.username)\n",
    "password.clear()\n",
    "password.send_keys(config.password)\n",
    "\n",
    "#target the login button and click it\n",
    "button = WebDriverWait(driver, 2).until(EC.element_to_be_clickable((By.CSS_SELECTOR, \"button[type='submit']\"))).click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9ee85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait up to 10 seconds for the search button to be clickable on the web page\n",
    "search_button = WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.CSS_SELECTOR, 'svg[aria-label=\"Search\"]')))\n",
    "\n",
    "# Click the search button once it becomes clickable\n",
    "search_button.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed26dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#target the search input field\n",
    "searchbox = WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.XPATH, \"//input[@placeholder='Search']\")))\n",
    "searchbox.clear()\n",
    "\n",
    "#search for the @handle or keyword\n",
    "keyword = \"@sample_handle\"\n",
    "searchbox.send_keys(keyword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06c2e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the keyword starts with \"@\"\n",
    "if keyword.startswith(\"@\"):\n",
    "    # Remove the \"@\" symbol\n",
    "    keyword = keyword[1:]\n",
    "    \n",
    "# Find the first element with the specified XPath that matches the keyword    \n",
    "first_result = driver.find_element(By.XPATH, f'//span[text()=\"{keyword}\"]')\n",
    "\n",
    "# Click on the found element (assuming it represents the desired search result)\n",
    "first_result.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c79c06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the initial page height\n",
    "initial_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "# Create a list to store htmls\n",
    "soups = []\n",
    "\n",
    "while True:\n",
    "    # Scroll down to the bottom of the page\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "\n",
    "    # Wait for a moment to allow new content to load (adjust as needed)\n",
    "    time.sleep(5)\n",
    "    \n",
    "    # Parse the HTML\n",
    "    html = driver.page_source\n",
    "    \n",
    "    # Create a BeautifulSoup object from the scraped HTML\n",
    "    soups.append(BeautifulSoup(html, 'html.parser'))\n",
    "\n",
    "    # Get the current page height\n",
    "    current_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "    if current_height == initial_height:\n",
    "        break  # Exit the loop when you can't scroll further\n",
    "\n",
    "    initial_height = current_height  # Update the initial height for the next iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d3fda95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List to store the post image URLs\n",
    "post_urls = []\n",
    "\n",
    "for soup in soups:\n",
    "    # Find all image elements that match the specific class in the current soup\n",
    "    elements = soup.find_all('a', class_='x1i10hfl xjbqb8w x6umtig x1b1mbwd xaqea5y xav7gou x9f619 x1ypdohk xt0psk2 xe8uvvx xdj266r x11i5rnm xat24cr x1mh8g0r xexx8yu x4uap5 x18d9i69 xkhd6sd x16tdsg8 x1hl2dhg xggy1nq x1a2a7pz _a6hd')\n",
    "\n",
    "    # Extract the href attributes and filter URLs that start with \"/p/\" or \"/reel/\"\n",
    "    post_urls.extend([element['href'] for element in elements if element['href'].startswith((\"/p/\", \"/reel/\"))])\n",
    "    \n",
    "# Convert the list to a set to remove duplicates\n",
    "unique_post_urls = list(set(post_urls))\n",
    "\n",
    "print(f\"before: {len(post_urls)}, after:{len(unique_post_urls)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "801f4c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_list = []\n",
    "\n",
    "# Define the query parameters to add\n",
    "query_parameters = \"__a=1&__d=dis\"\n",
    "\n",
    "# go through all urls\n",
    "for url in unique_post_urls:\n",
    "    try:\n",
    "        # Get the current URL of the page\n",
    "        current_url = driver.current_url\n",
    "\n",
    "        # Append the query parameters to the current URL\n",
    "        modified_url = \"https://www.instagram.com/\" + url + \"?\" + query_parameters\n",
    "\n",
    "        # Get URL\n",
    "        driver.get(modified_url)\n",
    "\n",
    "        # Wait for a moment to allow new content to load (adjust as needed)\n",
    "        time.sleep(1)\n",
    "\n",
    "        # Find the <pre> tag containing the JSON data\n",
    "        WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.XPATH, '//pre'))\n",
    "        )\n",
    "        pre_tag = driver.find_element_by_xpath('//pre')\n",
    "\n",
    "        # Extract the JSON data from the <pre> tag\n",
    "        json_script = pre_tag.text\n",
    "\n",
    "        # Parse the JSON data\n",
    "        json_parsed = json.loads(json_script)\n",
    "\n",
    "        # Add json to the list\n",
    "        json_list.append(json_parsed)\n",
    "    except (NoSuchElementException, TimeoutException, json.JSONDecodeError) as e:\n",
    "        print(f\"Error processing URL {url}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c032fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lists to store URLs and corresponding dates\n",
    "all_urls = []\n",
    "all_dates = []\n",
    "\n",
    "# Iterate through each JSON data in the list\n",
    "for json_data in json_list:\n",
    "    \n",
    "    # Extract the list from the 'items' key\n",
    "    item_list = json_data.get('items', [])\n",
    "    \n",
    "    # Iterate through each item in the 'items' list\n",
    "    for item in item_list:\n",
    "        \n",
    "        # Extract the date the item was taken\n",
    "        date_taken = item.get('taken_at')  # Move this line inside the loop\n",
    "\n",
    "        # Check if 'carousel_media' is present\n",
    "        carousel_media = item.get('carousel_media', [])\n",
    "        \n",
    "        # Iterate through each media in the 'carousel_media' list\n",
    "        for media in carousel_media:\n",
    "            \n",
    "            # Extract the image URL from the media\n",
    "            image_url = media.get('image_versions2', {}).get('candidates', [{}])[0].get('url')\n",
    "            \n",
    "            if image_url:\n",
    "                # Add the image URL and corresponding date to the lists\n",
    "                all_urls.append(image_url)\n",
    "                all_dates.append(date_taken)\n",
    "                print(f\"carousel image added\")\n",
    "                \n",
    "            # Extract the video URL from the media\n",
    "            video_versions = media.get('video_versions', [])\n",
    "            if video_versions:\n",
    "                video_url = video_versions[0].get('url')\n",
    "                if video_url:\n",
    "                    \n",
    "                    # Add the video URL and corresponding date to the lists\n",
    "                    all_urls.append(video_url)\n",
    "                    all_dates.append(date_taken)\n",
    "                    print(f\"carousel video added\")\n",
    "\n",
    "        # Handle cases of unique image, instead of carousel\n",
    "        image_url = item.get('image_versions2', {}).get('candidates', [{}])[0].get('url')\n",
    "        if image_url:\n",
    "            \n",
    "            # Add the image URL and corresponding date to the lists\n",
    "            all_urls.append(image_url)\n",
    "            all_dates.append(date_taken)\n",
    "            print(f\"single image added\")\n",
    "\n",
    "        # Check if 'video_versions' key exists\n",
    "        video_versions = item.get('video_versions', [])\n",
    "        if video_versions:\n",
    "            video_url = video_versions[0].get('url')\n",
    "            if video_url:\n",
    "                all_urls.append(video_url)\n",
    "                all_dates.append(date_taken)\n",
    "                print(f\"video added\")\n",
    "                \n",
    "# Print or use all collected URLs as needed\n",
    "print(len(all_urls))\n",
    "                \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df201782",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a directory to store downloaded files\n",
    "download_dir = keyword\n",
    "os.makedirs(download_dir, exist_ok=True)\n",
    "\n",
    "# Create subfolders for images and videos\n",
    "image_dir = os.path.join(download_dir, \"images\")\n",
    "video_dir = os.path.join(download_dir, \"videos\")\n",
    "os.makedirs(image_dir, exist_ok=True)\n",
    "os.makedirs(video_dir, exist_ok=True)\n",
    "\n",
    "# Initialize counters for images and videos\n",
    "image_counter = 1\n",
    "video_counter = 1\n",
    "\n",
    "# Iterate through URLs in the all_urls list and download media\n",
    "for index, url in enumerate(all_urls, 0):\n",
    "    response = requests.get(url, stream=True)\n",
    "\n",
    "    # Extract file extension from the URL\n",
    "    url_path = urlparse(url).path\n",
    "    file_extension = os.path.splitext(url_path)[1]\n",
    "\n",
    "    # Determine the file name based on the URL\n",
    "    if file_extension.lower() in {'.jpg', '.jpeg', '.png', '.gif'}:\n",
    "        file_name = f\"{all_dates[index]}-img-{image_counter}.png\"\n",
    "        destination_folder = image_dir\n",
    "        image_counter += 1\n",
    "    elif file_extension.lower() in {'.mp4', '.avi', '.mkv', '.mov'}:\n",
    "        file_name = f\"{all_dates[index]}-vid-{video_counter}.mp4\"\n",
    "        destination_folder = video_dir\n",
    "        video_counter += 1\n",
    "    else:\n",
    "        # Default to the main download directory for other file types\n",
    "        file_name = f\"{all_dates[index]}{file_extension}\"\n",
    "        destination_folder = download_dir\n",
    "\n",
    "    # Save the file to the appropriate folder\n",
    "    file_path = os.path.join(destination_folder, file_name)\n",
    "    \n",
    "    # Write the content of the response to the file\n",
    "    with open(file_path, 'wb') as file:\n",
    "        for chunk in response.iter_content(chunk_size=8192):\n",
    "            if chunk:\n",
    "                file.write(chunk)\n",
    "\n",
    "    print(f\"Downloaded: {file_path}\")\n",
    "\n",
    "# Print a message indicating the number of downloaded files and the download directory\n",
    "print(f\"Downloaded {len(all_urls)} files to {download_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710738a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
